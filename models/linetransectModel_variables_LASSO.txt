
    model{
    
    #################
    #Detection model#
    #################
    
    pi <- 3.141593
    
    # priors for fixed effect parms for half-normal detection parm sigma
    b.df.0 ~ dunif(0,20)        
    b.group.size ~ dnorm(0,0.01)
    
    #adm random effect on detection
    for(i in 1:nadm){
      b.adm[i] ~ dnorm(0,b.adm.tau)
    }
    b.adm.sd ~ dt(0, pow(2.5,-2), 1)T(0,)
    b.adm.tau <- pow(b.adm.sd,-2)
    
    for(i in 1:ndetections){
    #linear predictor
    mu.df[i] <- b.df.0 + b.group.size * ln_GroupSize[i] + b.adm[detectionAdm[i]]
    
    # estimate of sd and var, given coeffs above
    sig.df[i] <- exp(mu.df[i])
    sig2.df[i] <- sig.df[i]*sig.df[i]
    
    # effective strip width
    esw[i] <- sqrt(pi * sig2.df[i] / 2) 
    f0[i] <- 1/esw[i] #assuming all detected on the line
    
    # LIKELIHOOD
    # using zeros trick
    L.f0[i] <- exp(-y[i]*y[i] / (2*sig2.df[i])) * 1/esw[i] #y are the distances
    nlogL.f0[i] <-  -log(L.f0[i])
    zeros.dist[i] ~ dpois(nlogL.f0[i])
    }

    #using this model and predicted group size (below), get predicted ESW 
    for(t in 1:nyear){
      for(j in 1:nsite){
        pred.sig[j,t] <- exp(b.df.0 + b.group.size * log(GroupSizes[j,t]+1) + b.adm[adm[j]]) 
        pred.sig2[j,t] <- pow(pred.sig[j,t],2)
        predESW[j,t] <- sqrt(pi * pred.sig2[j,t] / 2)
      }
    }

    ########################
    #Model of total density#
    ########################

    #intercept
    int.d ~ dnorm(0,0.001)    
    
    #random transect effect
    transect.d.sd ~ dt(0, pow(2.5,-2), 1)T(0,)
    transect.d.tau <- pow(transect.d.sd,-2)
    for(j in 1:nsite){
      random.d.transect[j] ~ dnorm(0,transect.d.tau)
    } 
     
    #random line effect - all lines
    line.d.sd ~ dt(0, pow(2.5,-2), 1)T(0,)
    line.d.tau <- pow(line.d.sd,-2)
    for(j in 1:nsiteAll){
      random.d.line[j] ~ dnorm(0,line.d.tau)
    }
    
    #random year/line effect
    yearline.d.sd ~ dt(0, pow(2.5,-2), 1)T(0,)
    yearline.d.tau <- pow(yearline.d.sd,-2)
    for(j in 1:nsiteAll){
      for(t in 1:nyear){
        random.d.yearline[j,t] ~ dnorm(0,yearline.d.tau)
      }
    }
    
    #random adm effect
    adm.d.sd ~ dt(0, pow(2.5,-2), 1)T(0,)
    adm.d.tau <- pow(adm.d.sd,-2)
    for(j in 1:nadm){
      random.d.adm[j] ~ dnorm(0,adm.d.tau)
    }
    
    #random year/adm effect
    yearadm.d.sd ~ dt(0, pow(2.5,-2), 1)T(0,)
    yearadm.d.tau <- pow(yearadm.d.sd,-2)
    for(j in 1:nadm){
      for(t in 1:nyear){
        random.d.yearadm[j,t] ~ dnorm(0,yearadm.d.tau)
      }
    }
    
    #random year effect
    year.d.sd ~ dt(0, pow(2.5,-2), 1)T(0,)
    year.d.tau <- pow(year.d.sd,-2)
    for(t in 1:nyear){
      random.d.year[t] ~ dnorm(0,year.d.tau)
    }

    #covariate effects - L1 regularization == a Laplace (double exponential) prior 
    #https://stats.stackexchange.com/questions/28609/regularized-bayesian-logistic-regression-in-jags
    for(i in 1:n.covs){
      beta[i] ~ ddexp(0, dd.tau)
    }
    dd.tau ~ dgamma(0.01, 0.01)

    #Observation model:
    r ~ dunif(0,50)
    for(j in 1:nsite){
      for(t in 1:nyear){
        
        NuIndivs[j,t] ~ dnegbin(p[j,t],r)
        
        p[j,t] <- r/(r+expNuIndivs[j,t])
        
        #NuIndivs[j,t] ~ dpois(expNuIndivs[j,t])
        
        expNuIndivs[j,t] <- Density.jt[j,t] * TransectLength[j,t]/1000 * predESW[j,t]/1000 * 2
        
      }
    }
    
    #State model
    for(j in 1:nsite){
      for(t in 1:nyear){
      
      #linear predictor on density
        log(Density.jt[j,t]) <- int.d + 
                            random.d.year[t] +
                            inprod(beta[],occDM[j,]) +
                            random.d.adm[adm[j]] +
                            random.d.transect[j] +
                            random.d.line[siteAll[j]] +
                            random.d.yearline[siteAll[j],t] +
                            random.d.yearadm[adm[j],t]
                            
                            

    }}
    
    #predict observations - original scale and density scale
    for(j in 1:nsite){
      meanDensity[j] <- mean(Density.jt[j,])
      
    }
  
    #mean density at grids across Norway
    for(j in 1:n.preds){
      for(t in 1:nyear){
      
      #for each grid
      log(Density.km.jt[j,t]) <- int.d + 
                            inprod(beta[],predDM[j,]) + 
                            random.d.adm[pred.adm[j]] +
                            random.d.line[j] +
                            random.d.year[t] +
                            random.d.yearline[j,t]+
                            random.d.yearadm[pred.adm[j],t]
                            
      }
      
      Density.km[j] <- mean(Density.km.jt[j,])#mean per year
      Density[j] <- Density.km[j]*25 # since there are 25 km per grid
    }

    #totalPop <- sum(Density)

    #calculate the Bayesian p-value - at the site-level
    
    #get expected values per site
    for(j in 1:nsite){
      for(t in 1:nyear){ 

        #expected value of model
        exp[j,t] <- (r*(1-p[j,t]))/p[j,t]
        #New samples
        NuIndivs.new[j,t] ~ dnegbin(p[j,t],r)
        
      }
      exp.j[j] <- mean(exp[j,])
      NuIndivs.j[j] <- mean(NuIndivs[j,])
      NuIndivs.new.j[j] <- mean(NuIndivs.new[j,])
    }
    
    e <- 0.0001
    
    #do discrepancy at the site-level
    for(j in 1:nsite){
        
        #compared to data
        chi2[j] <- pow((NuIndivs.j[j] - exp.j[j]),2) / (sqrt(exp.j[j])+e)
        
        #compared to simulated values
        chi2.new[j] <- pow((NuIndivs.new.j[j] - exp.j[j]),2) / (sqrt(exp.j[j])+e) 
      
    }
    
    # Add up discrepancy measures for entire data set
      fit <- sum(chi2[])                     
      fit.new <- sum(chi2.new[])             
    
    }
    
