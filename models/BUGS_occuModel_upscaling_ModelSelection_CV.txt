
  model{


  #### Training dataset ####

  # State model
  for (i in 1:nsite_train){ 
    for (t in 1:nyear_train){
    
      z[i,t] ~ dbern(muZ[i,t]) 
      
      logit(muZ[i,t])<- int.alpha + inprod(beta[],occDM_train[i,]) + 
                          #random.adm[adm_train[i]] + 
                          random.year[t] +
                          eta[i]
      
    }
  }   
  
  ### Observation Model
  for(j in 1:nvisit_train) {
  
    y_train[j] ~ dbern(Py_train[j]) #data is Y
    Py_train[j]<- z[site_train[j],year_train[j]]*p[j] 

    #detection model:
      logit(p[j]) <-  int.d + 
                      beta.effort * Effort_train[j] +
                      #random.det.adm[det.adm_train[j]]+
                      random.det.year[year_train[j]] 
                      
                      
    } 
  
  #### Testing dataset ####
  
  # State model
  for (i in 1:nsite_test){ 
    for (t in 1:nyear_test){
    
      z_test[i,t] ~ dbern(muZ_test[i,t]) 
      
      logit(muZ_test[i,t])<- int.alpha + inprod(beta[],occDM_test[i,]) + 
                          #random.adm[adm_test[i]] + 
                          random.year[t] 
      
    }
  }   
  
  ### Observation Model
  for(j in 1:nvisit_test) {

    #compare Py with y
    Py_test[j]<- z_test[site_test[j],year_test[j]]*p_test[j] 

    #detection model:
      logit(p_test[j]) <-  int.d + 
                      beta.effort * Effort_test[j] +
                      #random.det.adm[det.adm_test[j]] +
                      random.det.year[year_test[j]] 
                      
                      
    } 
  
  #pull out mid values 
  
  for(i in 1:nsite_train){
    mid.z_train[i] <- z[i,6]
    mid.psi_train[i] <- muZ[i,6]
  }
  
  for(i in 1:nsite_test){
    mid.z_test[i] <- z_test[i,6]
    mid.psi_test[i] <- muZ_test[i,6]
  }
  

  #Priors 

  # State model priors
    ############################
    
    #intercept
    mean.psi ~ dunif(0, 1)       
    int.alpha <- logit(mean.psi)
 
    #from Rushing et al.
    ## Priors for linear indicator variables -- prob = 1 if quadratic term in model, 0.5 otherwise
    for(i in 1:12){
      g[i] ~ dbern(0.5)
    }
    
    ## Priors for quadratic indicator variables - prob = 0.5
    for(i in 13:24){
      g[i] ~ dbern(g[i - 12] * 0.5)
    }
    
    ## Priors for betas 
    
    for(i in 1:24){ # need to change?
      #https://gist.github.com/oliviergimenez/8cffedd2defc32a6811d7e715b898d6a
      #simple approach
      betaT[i] ~ dnorm(0,0.01)
      beta[i] <- g[i] * betaT[i]
    
      #bit more complex
      #https://stackoverflow.com/questions/47758236/ssvs-and-spike-slab-prior-with-jags
      #beta[i] ~ dnorm(0,beta.tau[i])
      #beta.tau[i] <-(100*(1-g[i]))+(0.001*(g[i]))    #when gamma is 0, tau is 100 (sd = ), 
                                                  #when gamma is 1, 0.001 (sd = )  
    }
    
    #years
    for(t in 1:nyear_train){
      random.year[t] ~ dnorm(0, tau.a)
    }
    tau.a <- 1/(sd.a * sd.a)
    sd.a ~ dt(0, 1, 1)T(0,) 
    
    #site effects
    for (i in 1:nsite_train) {
      eta[i] ~ dnorm(0, tau2)       
    } 
    tau2 <- 1/(sigma2 * sigma2) 
    sigma2 ~ dt(0, 1, 1)T(0,) 

    #adm effects
    for(i in 1:n.adm_train){
      random.adm[i] ~ dnorm(0,random.adm.tau)
    }
    random.adm.tau <- pow(random.adm.sd,-2)
    random.adm.sd ~ dt(0, 1, 1)T(0,) 

    #Observation model priors
    ##########################
    mean.p ~ dunif(0, 1)        
    int.d <- logit(mean.p)
    
    #year effects
    for (t in 1:nyear_train) {
      random.det.year[t] ~ dnorm(0, tau.lp)            
    }
    tau.lp <- 1 / (sd.lp * sd.lp)                 
    sd.lp ~ dt(0, 1, 1)T(0,)  
    
    #covariate effects
    beta.det.open ~ dnorm(0,0.01)
    beta.effort ~ dnorm(0,0.01)
    
    #adm effects
    for(i in 1:n.adm_train){
      random.det.adm[i] ~ dnorm(0,random.det.adm.tau)
    }
    random.det.adm.tau <- pow(random.adm.sd,-2)
    random.det.adm.sd ~ dt(0, 1, 1)T(0,) 


  }
    
